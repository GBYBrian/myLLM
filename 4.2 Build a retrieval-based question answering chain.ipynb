{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613aa8b6",
   "metadata": {},
   "source": [
    "# 1.加载向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a52501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# using zhipuai's embedding API\n",
    "from zhipuai_embedding import ZhipuAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0098e23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'32b0876eda94b85d1d0081eafb5aad2e.1tUeCGG9ElLcJiAO'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']\n",
    "zhipuai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59b77a",
   "metadata": {},
   "source": [
    "加载向量数据库，其中包含多个文档的Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f0db225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量数据库中存储的数量：40\n"
     ]
    }
   ],
   "source": [
    "# define Embeddings\n",
    "embedding = ZhipuAIEmbeddings()\n",
    "# 向量数据库持久化路径\n",
    "persist_directory = '../LLM/data_base/vector_db/chroma'\n",
    "# 加载数据库\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory, # 允许我们将persist_directory目录保存到本地磁盘\n",
    "    embedding_function=embedding\n",
    "    # 提供embedding_function用于新数据的处理和查询\n",
    ")\n",
    "print(f\"向量数据库中存储的数量：{vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eed0ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容：\n",
      "当您使用指令微调 LLM 时，您可以类比为向另一个人提供指令（假设他很聪明但不知道您任务的具体细节）。因此，当 LLM 无法正常工作时，有时是因为指令不够清晰。例如，如果您想问“请为我写一些关于阿兰·图灵( Alan Turing )的东西”，在此基础上清楚表明您希望文本专注于他的科学工作、个人生活、历史角色或其他方面可能会更有帮助。另外您还可以指定回答的语调, 来更加满足您的需求，可选项包括专业记者写作，或者向朋友写的随笔等。\n",
      "\n",
      "如果你将 LLM 视为一名新毕业的大学生，要求他完成这个任务，你甚至可以提前指定他们应该阅读哪些文本片段来写关于 Alan Turing 的文本，这样能够帮助这位新毕业的大学生更好地完成这项任务。下一章你会看到提示词创建的两个原则，一是清晰明确，二是给LLM时间去思考。\n",
      "----------\n",
      "检索到的第1个内容：\n",
      "当您使用指令微调 LLM 时，您可以类比为向另一个人提供指令（假设他很聪明但不知道您任务的具体细节）。因此，当 LLM 无法正常工作时，有时是因为指令不够清晰。例如，如果您想问“请为我写一些关于阿兰·图灵( Alan Turing )的东西”，在此基础上清楚表明您希望文本专注于他的科学工作、个人生活、历史角色或其他方面可能会更有帮助。另外您还可以指定回答的语调, 来更加满足您的需求，可选项包括专业记者写作，或者向朋友写的随笔等。\n",
      "\n",
      "如果你将 LLM 视为一名新毕业的大学生，要求他完成这个任务，你甚至可以提前指定他们应该阅读哪些文本片段来写关于 Alan Turing 的文本，这样能够帮助这位新毕业的大学生更好地完成这项任务。下一章你会看到提示词创建的两个原则，一是清晰明确，二是给LLM时间去思考。\n",
      "----------\n",
      "检索到的第2个内容：\n",
      "互联网上有很多有关提示词（Prompt, 本教程中将保留该术语）的材料，例如《30 prompts everyone has to know》之类的文章。这些文章主要集中在 ChatGPT 的 Web 界面上，许多人在使用它执行特定的、通常是一次性的任务。但是，我认为对于开发人员，LLM 的更强大功能是能通过 API 调用，从而快速构建软件应用程序。我认为这方面还没有得到充分的重视。实际上，我们在 DeepLearning.AI 的姊妹公司 AI Fund 的团队一直在与许多初创公司合作，将这些技术应用于诸多应用程序上。很兴奋能看到 LLM API 能够让开发人员非常快速地构建应用程序。\n",
      "\n",
      "在本课程中，我们将与您分享一些技巧，来挖掘 LLM 的潜力，也会提供应用上的最佳实践。过程中会涉及大量材料。首先，你会学习到用于软件开发的 Prompt 最佳实践，随后会涉及到几个常用使用例，包括概括、推断、转换与扩展，最后会利用 LLM 构建 chatbot（聊天机器人）。希望这能激发你的想象力，去开拓新应用。\n",
      "----------\n",
      "检索到的第3个内容：\n",
      "互联网上有很多有关提示词（Prompt, 本教程中将保留该术语）的材料，例如《30 prompts everyone has to know》之类的文章。这些文章主要集中在 ChatGPT 的 Web 界面上，许多人在使用它执行特定的、通常是一次性的任务。但是，我认为对于开发人员，LLM 的更强大功能是能通过 API 调用，从而快速构建软件应用程序。我认为这方面还没有得到充分的重视。实际上，我们在 DeepLearning.AI 的姊妹公司 AI Fund 的团队一直在与许多初创公司合作，将这些技术应用于诸多应用程序上。很兴奋能看到 LLM API 能够让开发人员非常快速地构建应用程序。\n",
      "\n",
      "在本课程中，我们将与您分享一些技巧，来挖掘 LLM 的潜力，也会提供应用上的最佳实践。过程中会涉及大量材料。首先，你会学习到用于软件开发的 Prompt 最佳实践，随后会涉及到几个常用使用例，包括概括、推断、转换与扩展，最后会利用 LLM 构建 chatbot（聊天机器人）。希望这能激发你的想象力，去开拓新应用。\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "question = \"what means prompt engineering\"\n",
    "docs = vectordb.similarity_search(question,k = 4)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"检索到的第{i}个内容：\\n{doc.page_content}\",end=\"\\n----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebd069",
   "metadata": {},
   "source": [
    "# 2.Create a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74f06025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_AIP_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "BASE_URL = os.environ['BASE_URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81cfcac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, I am an AI digital assistant designed to help and assist with various tasks and inquiries. I can provide information, answer questions, and help with a wide range of topics. I am always here to assist you to the best of my abilities. Please feel free to ask me anything you need help with.', response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 10, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3ec17f54-bd57-40a2-98e4-c64934b5fd0b-0', usage_metadata={'input_tokens': 10, 'output_tokens': 62, 'total_tokens': 72})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0,\n",
    "                 openai_api_key = OPENAI_AIP_KEY,base_url = BASE_URL)\n",
    "llm.invoke(\"please introduce yourself\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0660bedd",
   "metadata": {},
   "source": [
    "# 3.构建检索问答链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8dc58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，\n",
    "不要试图编造答案。最多使用三句话。\n",
    "答案尽量简明扼要。总是要在回答的最后说“谢谢你的提问”。\n",
    "{context}\n",
    "问题：{question}\n",
    "\"\"\"\n",
    "# 创建一个提示模板实例\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f94788d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA.from_chain_type(# 指定使用的语言模型和检索器，并传递提示模板\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(), # 将vectordb转换为一个检索器，用于从数据库中检索相关的文档\n",
    "                                       \n",
    "    return_source_documents=True,     # 是否返回原文档\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT} # 指定了要使用的提示模板\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73439aa0",
   "metadata": {},
   "source": [
    "# 4.检索问答链效果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e03399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"hi\"\n",
    "question_2 = \"智谱AI是一家什么公司\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd3b146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答question_1的结果：\n",
      " 抱歉，我不知道答案。谢谢你的提问。\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\":question_1})\n",
    "print(\"大模型+知识库后回答question_1的结果：\\n\",result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "564f554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Software\\anaconda3\\envs\\AFT_new\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'南瓜书是一本关于计算机科学和编程的教材。它的名称来源于计算机科学领域的术语“南瓜书（Pumpkin Book）”，这个术语指代的是一本经典的教材《编译原理》（Compilers: Principles, Techniques, and Tools），因其封面描绘了一只戴着南瓜帽子的小猫而得名。因此，一般而言，当人们提到“南瓜书”时，他们指的是这本著名的编译原理教材。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM自己回答的效果\n",
    "prompt_template = \"\"\"please answer the question as followed:{}\"\"\".format(question_1)\n",
    "llm.predict(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0289802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答question_1的结果：\n",
      " 我不知道智谱AI是一家什么公司。谢谢你的提问。\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\":question_2})\n",
    "print(\"大模型+知识库后回答question_1的结果：\\n\",result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2524075a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'智谱AI是一家人工智能公司。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM自己回答的效果\n",
    "prompt_template = \"\"\"please answer the question as followed:{}\"\"\".format(question_2)\n",
    "llm.predict(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6231d",
   "metadata": {},
   "source": [
    "# 添加历史对话的记忆功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cdd0bb",
   "metadata": {},
   "source": [
    "## 1.Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56a6c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key = \"chat_history\",\n",
    "    return_messages = True   # 消息列表的形式返回聊天记录，而非单个字符\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15482f69",
   "metadata": {},
   "source": [
    "## 2.对话检索链"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a846b0d",
   "metadata": {},
   "source": [
    "对话检索链在检索QA链的基础上，增加了处理对话历史的能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8be118cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, you can learn the following specific details about fine-tuning:\n",
      "\n",
      "- The course will focus on the best practices for fine-tuning language models (LLM) and recommend using them in most scenarios.\n",
      "- The course materials have contributions from both OpenAI and DeepLearning.ai teams.\n",
      "- Several individuals from OpenAI, including Andrew Main, Joe Palermo, Boris Power, Ted Sanders, and Lillian Weng, were involved in brainstorming and reviewing the course materials.\n",
      "- Geoff Ladwig, Eddy Shyu, and Tommy Nelson from DeepLearning.ai have also worked on the course.\n",
      "\n",
      "Unfortunately, the given context does not provide further details about fine-tuning itself.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "retriever = vectordb.as_retriever() #向量数据库的检索器\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    retriever = retriever,\n",
    "    memory = memory\n",
    ")\n",
    "question = \"can I learn something about fine-tune\"\n",
    "result = qa({\"question\":question})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f55173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is important to have knowledge about fine-tuning in the context of this course because the course focuses on best practices for fine-tuning language models. It is recommended to use these practices in most scenarios. Fine-tuning allows us to adapt pre-trained models to specific tasks or domains, improving their performance and making them more useful in practical applications. So, understanding fine-tuning techniques is crucial for effectively utilizing language models in this course.\n"
     ]
    }
   ],
   "source": [
    "question = \"why we need the knowledge in this aspect\"\n",
    "result = qa({\"question\":question})\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AFT_new]",
   "language": "python",
   "name": "conda-env-AFT_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
